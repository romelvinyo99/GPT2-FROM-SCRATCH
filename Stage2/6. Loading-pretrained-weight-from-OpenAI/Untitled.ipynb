{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf8d0de-fc76-4446-adae-3e2b59ada6c1",
   "metadata": {},
   "source": [
    "## 1. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514ebfbd-b8e6-4a0b-a5f0-d04e26ee4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd72d76e-f7b6-47ea-805f-3dd1845402d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the GPT configuration settings\n",
    "\n",
    "GPT_Config_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a1effd-8ee3-4299-8b16-94a0b51c3e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\\n\\n\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it\\'s going to send the value of my picture \\'way up; but I don\\'t think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing\\'s lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn\\'s \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\\n\\nWell!--even through th'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data \n",
    "\n",
    "with open(r\"C:\\Users\\nyasa\\Downloads\\BUILDING LLM FROM SCRATCH\\Stage1\\1.Data Preparation and Sampling\\1.Tokenization\\1.Word-Based-Tokenization\\the-verdict.txt\", \"r\") as f:\n",
    "    text_data = f.read()\n",
    "text_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c960748-01aa-4d59-9950-779e47f0a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Input = [b, num_tokens, 768]\n",
    "# 1. Activation function\n",
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the GELU activation - Approximate formula\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0) / torch.pi) * (x + 0.44715 * torch.pow(x, 3))))\n",
    "        \n",
    "# 2. Layer normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # Defining the epsilon -  small constant added to the variance to prevent zero division - undefined - limits\n",
    "        self.eps = 1e-5\n",
    "        # Defining the scaling and shifting parameters - trainable - better results\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Getting the mean and variance of each row\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        # Getting the normalization values\n",
    "        norm_x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        # Returning the normalized values of x shifted and scaled - finetuning parameters\n",
    "        return self.scale * norm_x + self.shift\n",
    "        \n",
    "# 3. Feed forward        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4),\n",
    "            GELUActivation(),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# 4. Attention Mechanism\n",
    "# Creating the multi-head attention compact class\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout_rate, bias_units=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"dimensions out must be divisible by number of heads\"\n",
    "        # Getting the head dimensions\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        # Initializing the key query value weights - (d_out, d_out)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        # Initializing the final projection layer - optional - (d_out, d_out)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        # Creating the masking layer\n",
    "        self.register_buffer(\"mask\", torch.triu(\n",
    "            torch.ones(context_length, context_length),\n",
    "            diagonal = 1\n",
    "        ))\n",
    "        # Creating the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    # Forward pass    \n",
    "    def forward(self, x):\n",
    "        # Exploding the input shape\n",
    "        b, num_tokens, d_out = x.shape\n",
    "        # Getting the key query value matrices (b, num_tokens, d_out)\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values =  self.w_value(x)\n",
    "        # Reshaping the key query value matrices - (b, num_tokens, num_head, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Grouping by number of heads - (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # Getting the attention scores - (b, num_heads, num_tokens, num_tokens)\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "        # Masking the attention scores\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        # Scaling the attention scores\n",
    "        attention_scores = attention_scores / keys.shape[-1]**0.5\n",
    "        # Getting the attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Implementing the dropout layer\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # Getting the context vector - (b, num_heads, num_tokens, head_dim)\n",
    "        context_vector = attention_weights @ values\n",
    "        # Reshaping the context vectors - (b, num_tokens, num_heads, head_dim)\n",
    "        context_vector = context_vector.transpose(1, 2)\n",
    "        # Combining the result of mutiple heads - d_out = num_heads * head_dim\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, d_out)\n",
    "        # Passing the final context vector into the projection layer - optional\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "        return context_vector\n",
    "# Transformer class        \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Defining the normalization layers\n",
    "        self.layerNorm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.layerNorm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # Defining the dropout layers\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "        # Defining the Multi-Head Attention layer\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            dropout_rate = cfg[\"dropout_rate\"],\n",
    "            bias_units = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # Deefining the feed forward layer\n",
    "        self.feed_forward = FeedForward(cfg)\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut1 = x\n",
    "        x = self.layerNorm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        # Add the original input the output \n",
    "        x = x +  shortcut1\n",
    "        # Shortcut connection for the \n",
    "        shortcut2 = x\n",
    "        x = self.layerNorm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        # Add the original output\n",
    "        x = x + shortcut2\n",
    "        return x\n",
    "# Gpt model        \n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Defining the token embedding layer\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # Defining the positional embedding layer\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # Defining the dropout layer \n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "        # Defining the transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        # Defining the final normalization layer\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # Defining the final linear layer\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    # Forward pass\n",
    "    def forward(self, in_idx):\n",
    "        # Explosion of shape\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        # Getting the token embeddings\n",
    "        token_embeddings = self.tok_emb(in_idx)\n",
    "        # Getting the positional embeddings\n",
    "        positional_embeddings = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        # Getting the input embeddings\n",
    "        input_embeddings = token_embeddings + positional_embeddings\n",
    "        # Passing the input embeddings through the dropout layer\n",
    "        input_embeddings = self.drop_emb(input_embeddings)\n",
    "        # Passing the input embeddings through the transformer blocks\n",
    "        input_embeddings = self.transformer_blocks(input_embeddings)\n",
    "        # Passing the input embeddings through the final normalization layer\n",
    "        input_embeddings = self.final_norm(input_embeddings)\n",
    "        # Passint the input embeddings through the final linear layer to get logits\n",
    "        logits = self.out_head(input_embeddings)\n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04fb833f-b51e-487c-8f03-43d901584841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decoder and encoder functions\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# Encoder function\n",
    "def encoder(sample_text, tokenizer=tokenizer):\n",
    "    encoded_tensor = torch.tensor(tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "# Decoder function\n",
    "def decoder(encoded_tensor, tokenizer=tokenizer):\n",
    "    decoded_text = tokenizer.decode(encoded_tensor.squeeze(0).numpy())\n",
    "    return decoded_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd0219-060a-4952-b29c-294d30f52a8a",
   "metadata": {},
   "source": [
    "## 4. Comparing generation with decoding strategies and without decoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99481130-ae45-458c-8410-7b62899d6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weight_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8be4876-685c-4f54-b9d9-510468c11c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is upto dateC:\\Users\\nyasa\\Downloads\\gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = weight_download.download_and_load_gpt2(r\"C:\\Users\\nyasa\\Downloads\\gpt2\", \"124M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8049422-f01f-4d27-a92f-9f1c3575ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84fbffdd-3626-448f-9bbd-b119a0353ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0218caca-aa0e-40e5-a4d1-2a2208fb0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Assign and Check function\n",
    "def assign_and_check(left, right, name=\"\"):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"\\033[41mShape mismatch: {left.shape} != {right.shape}\\033[0m\")\n",
    "    \n",
    "    assigned_param = torch.nn.Parameter(torch.tensor(right))\n",
    "    is_equal = torch.allclose(assigned_param.data, torch.tensor(right), atol=1e-5)\n",
    "\n",
    "    print(f\"✅ Assignment successful for {name}: {is_equal}\")\n",
    "\n",
    "    return assigned_param\n",
    "\n",
    "# --- Weight Loading Function\n",
    "def load_gpt_weights_into_custom(gpt, params):\n",
    "    # Assign token and positional embeddings\n",
    "    gpt.tok_emb.weight = assign_and_check(gpt.tok_emb.weight, params[\"wte\"], name=\"tok_emb\")\n",
    "    gpt.pos_emb.weight = assign_and_check(gpt.pos_emb.weight, params[\"wpe\"], name=\"pos_emb\")\n",
    "\n",
    "    # Loop over blocks\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # Split weights into q, k, v\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "\n",
    "        # Attention weights\n",
    "        gpt.transformer_blocks[b].attention.w_query.weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_query.weight, q_w.T, name=f\"block{b}_w_query\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.w_key.weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_key.weight, k_w.T, name=f\"block{b}_w_key\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.w_value.weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_value.weight, v_w.T, name=f\"block{b}_w_value\"\n",
    "        )\n",
    "\n",
    "        # Split biases into q, k, v\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "\n",
    "        # Attention biases\n",
    "        gpt.transformer_blocks[b].attention.w_query.bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_query.bias, q_b, name=f\"block{b}_b_query\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.w_key.bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_key.bias, k_b, name=f\"block{b}_b_key\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.w_value.bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.w_value.bias, v_b, name=f\"block{b}_b_value\"\n",
    "        )\n",
    "\n",
    "        # Attention output projection\n",
    "        gpt.transformer_blocks[b].attention.out_proj.weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
    "            name=f\"block{b}_attn_out_proj_w\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.out_proj.bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].attention.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
    "            name=f\"block{b}_attn_out_proj_b\"\n",
    "        )\n",
    "\n",
    "        # Feed-forward first layer\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
    "            name=f\"block{b}_ffn_fc_w\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"],\n",
    "            name=f\"block{b}_ffn_fc_b\"\n",
    "        )\n",
    "\n",
    "        # Feed-forward second layer\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign_and_check(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
    "            name=f\"block{b}_ffn_proj_w\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign_and_check(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
    "            name=f\"block{b}_ffn_proj_b\"\n",
    "        )\n",
    "\n",
    "        # LayerNorm 1\n",
    "        gpt.transformer_blocks[b].layerNorm1.scale = assign_and_check(\n",
    "            gpt.transformer_blocks[b].layerNorm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"],\n",
    "            name=f\"block{b}_ln1_g\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].layerNorm1.shift = assign_and_check(\n",
    "            gpt.transformer_blocks[b].layerNorm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"],\n",
    "            name=f\"block{b}_ln1_b\"\n",
    "        )\n",
    "\n",
    "        # LayerNorm 2\n",
    "        gpt.transformer_blocks[b].layerNorm2.scale = assign_and_check(\n",
    "            gpt.transformer_blocks[b].layerNorm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"],\n",
    "            name=f\"block{b}_ln2_g\"\n",
    "        )\n",
    "        gpt.transformer_blocks[b].layerNorm2.shift = assign_and_check(\n",
    "            gpt.transformer_blocks[b].layerNorm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"],\n",
    "            name=f\"block{b}_ln2_b\"\n",
    "        )\n",
    "\n",
    "    # Final normalization layer\n",
    "    gpt.final_norm.scale = assign_and_check(\n",
    "        gpt.final_norm.scale,\n",
    "        params[\"g\"],\n",
    "        name=\"final_norm_g\"\n",
    "    )\n",
    "    gpt.final_norm.shift = assign_and_check(\n",
    "        gpt.final_norm.shift,\n",
    "        params[\"b\"],\n",
    "        name=\"final_norm_b\"\n",
    "    )\n",
    "\n",
    "    # Final output head (weight tying with embeddings)\n",
    "    gpt.out_head.weight = assign_and_check(\n",
    "        gpt.out_head.weight,\n",
    "        params[\"wte\"],\n",
    "        name=\"out_head_weight\"\n",
    "    )\n",
    "\n",
    "    return gpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d321449-5dc5-45ef-93f1-d66169cc3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_custom = GPTModel(GPT_Config_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b194ea7-f239-4afc-a0e6-81e27e63c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Assignment successful for tok_emb: True\n",
      "✅ Assignment successful for pos_emb: True\n",
      "✅ Assignment successful for block0_w_query: True\n",
      "✅ Assignment successful for block0_w_key: True\n",
      "✅ Assignment successful for block0_w_value: True\n",
      "✅ Assignment successful for block0_b_query: True\n",
      "✅ Assignment successful for block0_b_key: True\n",
      "✅ Assignment successful for block0_b_value: True\n",
      "✅ Assignment successful for block0_attn_out_proj_w: True\n",
      "✅ Assignment successful for block0_attn_out_proj_b: True\n",
      "✅ Assignment successful for block0_ffn_fc_w: True\n",
      "✅ Assignment successful for block0_ffn_fc_b: True\n",
      "✅ Assignment successful for block0_ffn_proj_w: True\n",
      "✅ Assignment successful for block0_ffn_proj_b: True\n",
      "✅ Assignment successful for block0_ln1_g: True\n",
      "✅ Assignment successful for block0_ln1_b: True\n",
      "✅ Assignment successful for block0_ln2_g: True\n",
      "✅ Assignment successful for block0_ln2_b: True\n",
      "✅ Assignment successful for block1_w_query: True\n",
      "✅ Assignment successful for block1_w_key: True\n",
      "✅ Assignment successful for block1_w_value: True\n",
      "✅ Assignment successful for block1_b_query: True\n",
      "✅ Assignment successful for block1_b_key: True\n",
      "✅ Assignment successful for block1_b_value: True\n",
      "✅ Assignment successful for block1_attn_out_proj_w: True\n",
      "✅ Assignment successful for block1_attn_out_proj_b: True\n",
      "✅ Assignment successful for block1_ffn_fc_w: True\n",
      "✅ Assignment successful for block1_ffn_fc_b: True\n",
      "✅ Assignment successful for block1_ffn_proj_w: True\n",
      "✅ Assignment successful for block1_ffn_proj_b: True\n",
      "✅ Assignment successful for block1_ln1_g: True\n",
      "✅ Assignment successful for block1_ln1_b: True\n",
      "✅ Assignment successful for block1_ln2_g: True\n",
      "✅ Assignment successful for block1_ln2_b: True\n",
      "✅ Assignment successful for block2_w_query: True\n",
      "✅ Assignment successful for block2_w_key: True\n",
      "✅ Assignment successful for block2_w_value: True\n",
      "✅ Assignment successful for block2_b_query: True\n",
      "✅ Assignment successful for block2_b_key: True\n",
      "✅ Assignment successful for block2_b_value: True\n",
      "✅ Assignment successful for block2_attn_out_proj_w: True\n",
      "✅ Assignment successful for block2_attn_out_proj_b: True\n",
      "✅ Assignment successful for block2_ffn_fc_w: True\n",
      "✅ Assignment successful for block2_ffn_fc_b: True\n",
      "✅ Assignment successful for block2_ffn_proj_w: True\n",
      "✅ Assignment successful for block2_ffn_proj_b: True\n",
      "✅ Assignment successful for block2_ln1_g: True\n",
      "✅ Assignment successful for block2_ln1_b: True\n",
      "✅ Assignment successful for block2_ln2_g: True\n",
      "✅ Assignment successful for block2_ln2_b: True\n",
      "✅ Assignment successful for block3_w_query: True\n",
      "✅ Assignment successful for block3_w_key: True\n",
      "✅ Assignment successful for block3_w_value: True\n",
      "✅ Assignment successful for block3_b_query: True\n",
      "✅ Assignment successful for block3_b_key: True\n",
      "✅ Assignment successful for block3_b_value: True\n",
      "✅ Assignment successful for block3_attn_out_proj_w: True\n",
      "✅ Assignment successful for block3_attn_out_proj_b: True\n",
      "✅ Assignment successful for block3_ffn_fc_w: True\n",
      "✅ Assignment successful for block3_ffn_fc_b: True\n",
      "✅ Assignment successful for block3_ffn_proj_w: True\n",
      "✅ Assignment successful for block3_ffn_proj_b: True\n",
      "✅ Assignment successful for block3_ln1_g: True\n",
      "✅ Assignment successful for block3_ln1_b: True\n",
      "✅ Assignment successful for block3_ln2_g: True\n",
      "✅ Assignment successful for block3_ln2_b: True\n",
      "✅ Assignment successful for block4_w_query: True\n",
      "✅ Assignment successful for block4_w_key: True\n",
      "✅ Assignment successful for block4_w_value: True\n",
      "✅ Assignment successful for block4_b_query: True\n",
      "✅ Assignment successful for block4_b_key: True\n",
      "✅ Assignment successful for block4_b_value: True\n",
      "✅ Assignment successful for block4_attn_out_proj_w: True\n",
      "✅ Assignment successful for block4_attn_out_proj_b: True\n",
      "✅ Assignment successful for block4_ffn_fc_w: True\n",
      "✅ Assignment successful for block4_ffn_fc_b: True\n",
      "✅ Assignment successful for block4_ffn_proj_w: True\n",
      "✅ Assignment successful for block4_ffn_proj_b: True\n",
      "✅ Assignment successful for block4_ln1_g: True\n",
      "✅ Assignment successful for block4_ln1_b: True\n",
      "✅ Assignment successful for block4_ln2_g: True\n",
      "✅ Assignment successful for block4_ln2_b: True\n",
      "✅ Assignment successful for block5_w_query: True\n",
      "✅ Assignment successful for block5_w_key: True\n",
      "✅ Assignment successful for block5_w_value: True\n",
      "✅ Assignment successful for block5_b_query: True\n",
      "✅ Assignment successful for block5_b_key: True\n",
      "✅ Assignment successful for block5_b_value: True\n",
      "✅ Assignment successful for block5_attn_out_proj_w: True\n",
      "✅ Assignment successful for block5_attn_out_proj_b: True\n",
      "✅ Assignment successful for block5_ffn_fc_w: True\n",
      "✅ Assignment successful for block5_ffn_fc_b: True\n",
      "✅ Assignment successful for block5_ffn_proj_w: True\n",
      "✅ Assignment successful for block5_ffn_proj_b: True\n",
      "✅ Assignment successful for block5_ln1_g: True\n",
      "✅ Assignment successful for block5_ln1_b: True\n",
      "✅ Assignment successful for block5_ln2_g: True\n",
      "✅ Assignment successful for block5_ln2_b: True\n",
      "✅ Assignment successful for block6_w_query: True\n",
      "✅ Assignment successful for block6_w_key: True\n",
      "✅ Assignment successful for block6_w_value: True\n",
      "✅ Assignment successful for block6_b_query: True\n",
      "✅ Assignment successful for block6_b_key: True\n",
      "✅ Assignment successful for block6_b_value: True\n",
      "✅ Assignment successful for block6_attn_out_proj_w: True\n",
      "✅ Assignment successful for block6_attn_out_proj_b: True\n",
      "✅ Assignment successful for block6_ffn_fc_w: True\n",
      "✅ Assignment successful for block6_ffn_fc_b: True\n",
      "✅ Assignment successful for block6_ffn_proj_w: True\n",
      "✅ Assignment successful for block6_ffn_proj_b: True\n",
      "✅ Assignment successful for block6_ln1_g: True\n",
      "✅ Assignment successful for block6_ln1_b: True\n",
      "✅ Assignment successful for block6_ln2_g: True\n",
      "✅ Assignment successful for block6_ln2_b: True\n",
      "✅ Assignment successful for block7_w_query: True\n",
      "✅ Assignment successful for block7_w_key: True\n",
      "✅ Assignment successful for block7_w_value: True\n",
      "✅ Assignment successful for block7_b_query: True\n",
      "✅ Assignment successful for block7_b_key: True\n",
      "✅ Assignment successful for block7_b_value: True\n",
      "✅ Assignment successful for block7_attn_out_proj_w: True\n",
      "✅ Assignment successful for block7_attn_out_proj_b: True\n",
      "✅ Assignment successful for block7_ffn_fc_w: True\n",
      "✅ Assignment successful for block7_ffn_fc_b: True\n",
      "✅ Assignment successful for block7_ffn_proj_w: True\n",
      "✅ Assignment successful for block7_ffn_proj_b: True\n",
      "✅ Assignment successful for block7_ln1_g: True\n",
      "✅ Assignment successful for block7_ln1_b: True\n",
      "✅ Assignment successful for block7_ln2_g: True\n",
      "✅ Assignment successful for block7_ln2_b: True\n",
      "✅ Assignment successful for block8_w_query: True\n",
      "✅ Assignment successful for block8_w_key: True\n",
      "✅ Assignment successful for block8_w_value: True\n",
      "✅ Assignment successful for block8_b_query: True\n",
      "✅ Assignment successful for block8_b_key: True\n",
      "✅ Assignment successful for block8_b_value: True\n",
      "✅ Assignment successful for block8_attn_out_proj_w: True\n",
      "✅ Assignment successful for block8_attn_out_proj_b: True\n",
      "✅ Assignment successful for block8_ffn_fc_w: True\n",
      "✅ Assignment successful for block8_ffn_fc_b: True\n",
      "✅ Assignment successful for block8_ffn_proj_w: True\n",
      "✅ Assignment successful for block8_ffn_proj_b: True\n",
      "✅ Assignment successful for block8_ln1_g: True\n",
      "✅ Assignment successful for block8_ln1_b: True\n",
      "✅ Assignment successful for block8_ln2_g: True\n",
      "✅ Assignment successful for block8_ln2_b: True\n",
      "✅ Assignment successful for block9_w_query: True\n",
      "✅ Assignment successful for block9_w_key: True\n",
      "✅ Assignment successful for block9_w_value: True\n",
      "✅ Assignment successful for block9_b_query: True\n",
      "✅ Assignment successful for block9_b_key: True\n",
      "✅ Assignment successful for block9_b_value: True\n",
      "✅ Assignment successful for block9_attn_out_proj_w: True\n",
      "✅ Assignment successful for block9_attn_out_proj_b: True\n",
      "✅ Assignment successful for block9_ffn_fc_w: True\n",
      "✅ Assignment successful for block9_ffn_fc_b: True\n",
      "✅ Assignment successful for block9_ffn_proj_w: True\n",
      "✅ Assignment successful for block9_ffn_proj_b: True\n",
      "✅ Assignment successful for block9_ln1_g: True\n",
      "✅ Assignment successful for block9_ln1_b: True\n",
      "✅ Assignment successful for block9_ln2_g: True\n",
      "✅ Assignment successful for block9_ln2_b: True\n",
      "✅ Assignment successful for block10_w_query: True\n",
      "✅ Assignment successful for block10_w_key: True\n",
      "✅ Assignment successful for block10_w_value: True\n",
      "✅ Assignment successful for block10_b_query: True\n",
      "✅ Assignment successful for block10_b_key: True\n",
      "✅ Assignment successful for block10_b_value: True\n",
      "✅ Assignment successful for block10_attn_out_proj_w: True\n",
      "✅ Assignment successful for block10_attn_out_proj_b: True\n",
      "✅ Assignment successful for block10_ffn_fc_w: True\n",
      "✅ Assignment successful for block10_ffn_fc_b: True\n",
      "✅ Assignment successful for block10_ffn_proj_w: True\n",
      "✅ Assignment successful for block10_ffn_proj_b: True\n",
      "✅ Assignment successful for block10_ln1_g: True\n",
      "✅ Assignment successful for block10_ln1_b: True\n",
      "✅ Assignment successful for block10_ln2_g: True\n",
      "✅ Assignment successful for block10_ln2_b: True\n",
      "✅ Assignment successful for block11_w_query: True\n",
      "✅ Assignment successful for block11_w_key: True\n",
      "✅ Assignment successful for block11_w_value: True\n",
      "✅ Assignment successful for block11_b_query: True\n",
      "✅ Assignment successful for block11_b_key: True\n",
      "✅ Assignment successful for block11_b_value: True\n",
      "✅ Assignment successful for block11_attn_out_proj_w: True\n",
      "✅ Assignment successful for block11_attn_out_proj_b: True\n",
      "✅ Assignment successful for block11_ffn_fc_w: True\n",
      "✅ Assignment successful for block11_ffn_fc_b: True\n",
      "✅ Assignment successful for block11_ffn_proj_w: True\n",
      "✅ Assignment successful for block11_ffn_proj_b: True\n",
      "✅ Assignment successful for block11_ln1_g: True\n",
      "✅ Assignment successful for block11_ln1_b: True\n",
      "✅ Assignment successful for block11_ln2_g: True\n",
      "✅ Assignment successful for block11_ln2_b: True\n",
      "✅ Assignment successful for final_norm_g: True\n",
      "✅ Assignment successful for final_norm_b: True\n",
      "✅ Assignment successful for out_head_weight: True\n"
     ]
    }
   ],
   "source": [
    "gpt_openAI = load_gpt_weights_into_custom(gpt_custom, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "223b8a59-30fb-4ff0-a592-c090e4229ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temp=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Slicing to context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # Getting the logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # Getting the logits for the last prediction task\n",
    "        logits = logits[:, -1, :]\n",
    "        # Applying top-k sampling\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(input=logits, k=top_k, dim=-1)\n",
    "            # Getting the smallest top logit value - [b, logits]\n",
    "            min_val = top_logits[-1].min()\n",
    "            # -Infinity masking\n",
    "            logits = torch.where(\n",
    "                condition = logits < min_val,\n",
    "                input = torch.tensor(-torch.inf).to(logits.device),\n",
    "                other = logits\n",
    "            )\n",
    "        # Applying temprature scaling\n",
    "        if temp > 0.0:\n",
    "           logits = logits / temp\n",
    "           # Applying softmax\n",
    "           probabilities = torch.softmax(logits, dim=-1)\n",
    "           # Getting the next token\n",
    "           idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "        # If temprature is None - use argmax    \n",
    "        else:\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "        # If end of sequence token is encountered end sequence early    \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        # Updating the idx - input\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    # Returning the initial plus predicted tokens    \n",
    "    return idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "892df4cf-ebc9-4bd8-a470-409d09baa03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is business.\" This was a common complaint:\\n\\nWhat did the White House, Office of N. Strategy and the Office of Nidescoping Services, do. At one time or the risk is this, the EPA is investigating to what degree of pollution'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "sample_text = \"what is business\"\n",
    "token_ids = torch.tensor(tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})).unsqueeze(dim=0)\n",
    "ids = generate(\n",
    "    model = gpt_openAI,\n",
    "    idx=token_ids,\n",
    "    max_new_tokens=50, \n",
    "    context_size = 1024,\n",
    "    temp = 1.4, \n",
    "    top_k=25\n",
    ").squeeze(0).numpy()\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023ea212-83e9-4b20-9a91-02740700bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "torch.save(gpt_openAI.state_dict(), r\"C:\\Users\\nyasa\\Downloads\\gpt2\\gpt_openAI.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
