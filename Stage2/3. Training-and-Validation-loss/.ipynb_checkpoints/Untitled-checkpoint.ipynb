{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf8d0de-fc76-4446-adae-3e2b59ada6c1",
   "metadata": {},
   "source": [
    "## 1. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514ebfbd-b8e6-4a0b-a5f0-d04e26ee4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd72d76e-f7b6-47ea-805f-3dd1845402d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the GPT configuration settings\n",
    "\n",
    "GPT_Config_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6653dad-9e79-445a-9857-4e1c6d473919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Activation function\n",
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the GELU activation - Approximate formula\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0) / torch.pi) * (x + 0.44715 * torch.pow(x, 3))))\n",
    "        \n",
    "# 2. Layer normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # Defining the epsilon -  small constant added to the variance to prevent zero division - undefined - limits\n",
    "        self.eps = 1e-5\n",
    "        # Defining the scaling and shifting parameters - trainable - better results\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Getting the mean and variance of each row\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True)\n",
    "        # Getting the normalization values\n",
    "        norm_x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        # Returning the normalized values of x shifted and scaled - finetuning parameters\n",
    "        return self.scale * norm_x + self.shift\n",
    "        \n",
    "# 3. Feed forward        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4),\n",
    "            GELUActivation(),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# 4. Attention Mechanism\n",
    "# Creating the multi-head attention compact class\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout_rate, bias_units=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"dimensions out must be divisible by number of heads\"\n",
    "        # Getting the head dimensions\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        # Initializing the key query value weights - (d_out, d_out)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        # Initializing the final projection layer - optional - (d_out, d_out)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        # Creating the masking layer\n",
    "        self.register_buffer(\"mask\", torch.triu(\n",
    "            torch.ones(context_length, context_length),\n",
    "            diagonal = 1\n",
    "        ))\n",
    "        # Creating the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    # Forward pass    \n",
    "    def forward(self, x):\n",
    "        # Exploding the input shape\n",
    "        b, num_tokens, d_out = x.shape\n",
    "        # Getting the key query value matrices (b, num_tokens, d_out)\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values =  self.w_value(x)\n",
    "        # Reshaping the key query value matrices - (b, num_tokens, num_head, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Grouping by number of heads - (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # Getting the attention scores - (b, num_heads, num_tokens, num_tokens)\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "        # Masking the attention scores\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        # Scaling the attention scores\n",
    "        attention_scores = attention_scores / keys.shape[-1]**0.5\n",
    "        # Getting the attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Implementing the dropout layer\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # Getting the context vector - (b, num_heads, num_tokens, head_dim)\n",
    "        context_vector = attention_weights @ values\n",
    "        # Reshaping the context vectors - (b, num_tokens, num_heads, head_dim)\n",
    "        context_vector = context_vector.transpose(1, 2)\n",
    "        # Combining the result of mutiple heads - d_out = num_heads * head_dim\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, d_out)\n",
    "        # Passing the final context vector into the projection layer - optional\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c960748-01aa-4d59-9950-779e47f0a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Input = [b, num_tokens, 768]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Defining the normalization layers\n",
    "        self.layerNorm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.layerNorm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # Defining the dropout layers\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "        # Defining the Multi-Head Attention layer\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            dropout_rate = cfg[\"dropout_rate\"],\n",
    "            bias_units = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # Deefining the feed forward layer\n",
    "        self.feed_forward = FeedForward(cfg)\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.layerNorm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        # Add the original input the output \n",
    "        x = x +  shortcut\n",
    "        # Shortcut connection for the \n",
    "        x = shortcut\n",
    "        x = self.layerNorm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        # Add the original output\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9ddb0-d72f-4412-9b62-b13b473b0def",
   "metadata": {},
   "source": [
    "## 2. GPT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a532b71-add0-4c6a-95fb-b8e9ad5d2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Defining the token embedding layer\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # Defining the positional embedding layer\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # Defining the dropout layer \n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "        # Defining the transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        # Defining the final normalization layer\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # Defining the final linear layer\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    # Forward pass\n",
    "    def forward(self, in_idx):\n",
    "        # Explosion of shape\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        # Getting the token embeddings\n",
    "        token_embeddings = self.tok_emb(in_idx)\n",
    "        # Getting the positional embeddings\n",
    "        positional_embeddings = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        # Getting the input embeddings\n",
    "        input_embeddings = token_embeddings + positional_embeddings\n",
    "        # Passing the input embeddings through the dropout layer\n",
    "        input_embeddings = self.drop_emb(input_embeddings)\n",
    "        # Passing the input embeddings through the transformer blocks\n",
    "        input_embeddings = self.transformer_blocks(input_embeddings)\n",
    "        # Passing the input embeddings through the final normalization layer\n",
    "        input_embeddings = self.final_norm(input_embeddings)\n",
    "        # Passint the input embeddings through the final linear layer to get logits\n",
    "        logits = self.out_head(input_embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b7e23a-b717-4279-be52-4a2e4c616700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Example of instantiation\n",
    "\n",
    "sample_batch = torch.tensor([[6109, 3636, 6100, 345],[6109, 1110, 6622, 257]])\n",
    "print(sample_batch.shape)\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_Config_124M)\n",
    "out = model(sample_batch)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d877705d-b9b3-4e9d-9f4e-a528e37741fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162419712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of parameters\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158c4a4-ff26-4281-b3e8-294f2f89599c",
   "metadata": {},
   "source": [
    "- We can see this is not same as the original plan of 124M instead we have 163M\n",
    "- This is due to a concept weight tying\n",
    "- The GPT architechts reused the weight for the token embedding layer in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a160dc4-bb39-47e5-8192-b8c4bfbcc089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming shapes match\n",
    "\n",
    "model.out_head.weight.shape == model.tok_emb.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b7aa64-fefb-4d28-8e43-3a8d6f7dab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123822336"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the theory of weight reusal\n",
    "\n",
    "GPT_MODEL_PARAMS = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "GPT_MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3164776-333e-4636-8c25-b9e6e8dbb5ca",
   "metadata": {},
   "source": [
    "## 3.Generating output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f37464-711c-4356-9fe1-646e052b9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LLM book for further clarification\n",
    "\n",
    "def generate_tokens_simple(model, idx, max_num_tokens, context_size):\n",
    "    # Number of iterations - number of new tokens to be generated\n",
    "    for _ in range(max_num_tokens):\n",
    "        # idx.shape = [batch, num_tokens] e.g = [2, 2]\n",
    "        # Slicing - making sure the context size = 5 if not select last 5 tokens\n",
    "        idx_condition = idx[:, -5:]\n",
    "        # Making predictions - getting the logits - logits.shape = [b, num_tokens, vocab_size]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_condition)\n",
    "        # Extracting the last token row - logits[:, -1, :].shape = [b, 1, vocab_size]\n",
    "        logits = logits[:, -1, :]\n",
    "        # Converting the logits into probabilities - exponential sum\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        # Getting the element with the highest probability\n",
    "        idx_next = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "        # Appending the predicted token to the running sequence\n",
    "        # idx = [b, num_tokens+1]\n",
    "        # We are concatenating along the last dimension which in this case is the last dimension\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    # Returning the full sequence = original + generated upto max number of tokens\n",
    "    return idx   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b4144-0290-4ce3-bff6-cf99789e8332",
   "metadata": {},
   "source": [
    "## Testing Generation using real text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056aa2f6-6dc5-42fb-8f20-dcebf525143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text =  [15496, 11, 314, 716, 12806, 422, 25935]\n",
      "Encoded tensor =  tensor([[15496,    11,   314,   716, 12806,   422, 25935]]) Shape =  torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "# Encoding text to token ids\n",
    "import tiktoken\n",
    "\n",
    "starting_context = \"Hello, I am Jacob from Congo\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(starting_context)\n",
    "print(\"Encoded text = \", encoded)\n",
    "# Converting the encode into a tensor\n",
    "# The unsqueeze - we are adding an dimension: [num_tokens] ---> [b, num_tokens]\n",
    "encoded = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"Encoded tensor = \", encoded, \"Shape = \", encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7729591-1867-4aec-94c8-88ae585948bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor = tensor([[15496,    11,   314,   716, 12806,   422, 25935, 30543, 15508, 29900,\n",
      "         37257, 13825, 26687]]) Output tensor shape = torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "# Generating text using our model\n",
    "\n",
    "# Setting the evaluation/testing mode - cancels some unnecessary components e.g dropout layers\n",
    "model.eval()\n",
    "output_tensor = generate_tokens_simple(\n",
    "    model = model,\n",
    "    idx = encoded,\n",
    "    max_num_tokens = 6,\n",
    "    context_size = GPT_Config_124M[\"context_length\"]\n",
    ")\n",
    "print(f\"Output tensor = {output_tensor} Output tensor shape = {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1eeaaf3-308e-44d7-ad60-5f9397e1e81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am Jacob from Congo\"\\' teens undergone tightened obesityReference'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding the output tensor - remember we need to squeeze the first batch dimension\n",
    "# Note that the tiktoken doesnt work with tensor so use python lists or numpy arrays\n",
    "\n",
    "decoded_text = tokenizer.decode(output_tensor.squeeze(0).numpy())\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef205ce8-6d23-4a88-b00a-a333b31ab97c",
   "metadata": {},
   "source": [
    "## Creating decode and encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04fb833f-b51e-487c-8f03-43d901584841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decoder and encoder functions\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# Encoder function\n",
    "def encoder(sample_text, tokenizer=tokenizer):\n",
    "    encoded_tensor = torch.tensor(tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "# Decoder function\n",
    "def decoder(encoded_tensor, tokenizer=tokenizer):\n",
    "    decoded_text = tokenizer.decode(encoded_tensor.squeeze(0).numpy())\n",
    "    return decoded_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a409c9c-ad99-4b51-8dec-4b9c269584f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3626,  6100,   345, 35300, 29583, 24889,  7113, 39862, 30189]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the class\n",
    "sample_text = \"Every effort moves you\"\n",
    "token_ids = generate_tokens_simple(\n",
    "    model = model,\n",
    "    idx = encoder(sample_text, tokenizer),\n",
    "    max_num_tokens = 6,\n",
    "    context_size = GPT_Config_124M[\"context_length\"]\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa6f6b-82bc-4fea-918b-d6f522e316b1",
   "metadata": {},
   "source": [
    "## Creating Input - target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e409e900-6751-417f-94aa-adfb7e8e63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# effort moves you -- I really like\n",
    "inputs = torch.tensor([[16833, 3626, 6100],[40, 1107, 588]])\n",
    "# moves you forward -- really like chocolate\n",
    "targets = torch.tensor([[3626, 6100, 345], [1107, 588, 11311]])\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd120b0-77d2-423e-abd9-9c895c894a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3570,  0.6767, -0.5159,  ..., -0.6060,  0.3366, -0.0650],\n",
       "          [ 0.0936,  0.2446, -0.0886,  ..., -0.4846, -0.0741,  1.2275],\n",
       "          [-0.5307, -0.0697,  0.5274,  ..., -0.0065, -1.1845,  0.6131]],\n",
       " \n",
       "         [[-0.0906,  0.5485,  0.2573,  ...,  0.1525,  0.2995,  0.7435],\n",
       "          [-0.3525,  0.4848, -0.1509,  ..., -0.0295,  0.4599,  1.4260],\n",
       "          [ 0.7239, -0.8283,  0.3101,  ..., -0.5223, -0.7993, -1.4578]]]),\n",
       " torch.Size([2, 3, 50257]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predictions for the inputs - We have a third row which is the prediction\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "logits, logits.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09419e09-e097-4834-b463-09c96001c122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[42217],\n",
       "          [35523],\n",
       "          [ 5243]],\n",
       " \n",
       "         [[29716],\n",
       "          [ 8185],\n",
       "          [42456]]]),\n",
       " torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have three prediction tasks for every row but we will focus on the last one\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "outputs = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "outputs, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa9bd5ab-470c-4115-885b-0ca0b3c0b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42217, 35523,  5243, 29716,  8185, 42456])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4887d541-4ca7-4e46-9da1-be53b001dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  ceasesThumbnail radio\n"
     ]
    }
   ],
   "source": [
    "# Checking how far from true value we are\n",
    "\n",
    "print(f\"Targets batch 1: {decoder(targets[0].flatten())}\")\n",
    "print(f\"Outputs batch 1: {decoder(outputs[0].flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410bb3e-1538-4d72-b560-67e886be0a80",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca92bbac-f823-43e4-b51f-91a8ee1f2d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16833,  3626,  6100],\n",
       "         [   40,  1107,   588]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the inputs\n",
    "\n",
    "inputs, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "befcd3d7-fe48-4763-b4db-72525a3b7684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3626,  6100,   345],\n",
       "         [ 1107,   588, 11311]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the targets\n",
    "\n",
    "targets, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74549477-c6aa-483a-97e3-beef7cadea4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2.4135e-05, 3.3229e-05, 1.0082e-05,  ..., 9.2138e-06,\n",
       "           2.3649e-05, 1.5826e-05],\n",
       "          [1.8456e-05, 2.1465e-05, 1.5382e-05,  ..., 1.0352e-05,\n",
       "           1.5607e-05, 5.7356e-05],\n",
       "          [9.8770e-06, 1.5661e-05, 2.8455e-05,  ..., 1.6683e-05,\n",
       "           5.1364e-06, 3.1000e-05]],\n",
       " \n",
       "         [[1.5358e-05, 2.9099e-05, 2.1748e-05,  ..., 1.9584e-05,\n",
       "           2.2686e-05, 3.5367e-05],\n",
       "          [1.1778e-05, 2.7209e-05, 1.4408e-05,  ..., 1.6268e-05,\n",
       "           2.6540e-05, 6.9736e-05],\n",
       "          [3.4703e-05, 7.3491e-06, 2.2944e-05,  ..., 9.9808e-06,\n",
       "           7.5656e-06, 3.9162e-06]]]),\n",
       " torch.Size([2, 3, 50257]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the probabilities\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "probas, probas.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9562d-46f2-450f-ad36-c2abdf9e371a",
   "metadata": {},
   "source": [
    "A. Long Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ec70d94-7dee-4821-a45c-d30c6d3caa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.2132e-06, 4.8871e-05, 8.0817e-06]),\n",
       " tensor([1.8494e-05, 2.7398e-05, 7.3609e-06]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced indexing\n",
    "\n",
    "# Getting the probabilities for the first text\n",
    "text1_idx = 0\n",
    "text1_probas = probas[text1_idx, range(3), targets[text1_idx]]\n",
    "# Getting the probabilities for the second text\n",
    "text2_idx = 1\n",
    "text2_probas = probas[text2_idx, range(3), targets[text2_idx]]\n",
    "text1_probas, text2_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef92216-173b-48de-b290-8881a0bf9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged probabilities:  tensor([3.2132e-06, 4.8871e-05, 8.0817e-06, 1.8494e-05, 2.7398e-05, 7.3609e-06])\n",
      "Log of merged probabilities:  tensor([-12.6483,  -9.9263, -11.7259, -10.8981, -10.5050, -11.8193])\n",
      "Average log probability:  tensor(-11.2538)\n",
      "Negative average log probability:  tensor(11.2538)\n"
     ]
    }
   ],
   "source": [
    "# Merging the probabilities calculating the log probabilities\n",
    "text_probas = torch.cat((text1_probas, text2_probas), dim = -1)\n",
    "print(\"Merged probabilities: \", text_probas)\n",
    "# Calculating the log probabilities\n",
    "log_text_probas = torch.log(text_probas)\n",
    "print(\"Log of merged probabilities: \", log_text_probas)\n",
    "# Getting the average log probabilities value\n",
    "avg_log_probas = torch.mean(log_text_probas)\n",
    "print(\"Average log probability: \", avg_log_probas)\n",
    "# Getting the negative average log probabily value = loss\n",
    "loss = - avg_log_probas\n",
    "print(\"Negative average log probability: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98be218-d125-4623-901f-c55ca07194bc",
   "metadata": {},
   "source": [
    "B. Shortcut method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80360cce-ade0-40f5-a537-4bfa77f8b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets Original shape:  torch.Size([2, 3])\n",
      "New shapes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 50257]), torch.Size([6]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Original logits shape: \", logits.shape)\n",
    "print(\"Targets Original shape: \", targets.shape)\n",
    "# Flattening the first two dimension of the logits\n",
    "logits_flatten = logits.flatten(0, 1)\n",
    "# Flattening the targets \n",
    "targets_flatten = targets.flatten()\n",
    "print(\"New shapes\")\n",
    "logits_flatten.shape, targets_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d30a893-f583-4f09-b65b-9c065eaf84f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2538)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flatten, targets_flatten)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87286050-15a9-4140-a6ff-f4d58551febc",
   "metadata": {},
   "source": [
    "## Perplexity Score - Exponential of loss - Much more inteprable than loss\n",
    "\n",
    "- Lower perplexity score means a better model\n",
    "- **NB** A common misconception is that perplexity score ranges [1, num_classes] but not the case\n",
    "- Perplexity >= 1\n",
    "- The closer it is to one the better the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03124bdc-8f43-4b09-97d6-8a86a0efc904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77174.2656)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the perplexity score for our model so far\n",
    "\n",
    "perplexity_score = torch.exp(loss)\n",
    "perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb3fe251-09bd-4792-b074-9512b669944a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15496,   616,  1438,   318,  5616,    85,  2047,    13]]),\n",
       " tensor([[  40,  716,  257, 3710,   13]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing using a different input\n",
    "sample_inputs = \"Hello my name is Melvyn.\"\n",
    "sample_targets = \"I am a student.\"\n",
    "encoded_inputs = encoder(sample_inputs)\n",
    "encoded_targets = encoder(sample_targets)\n",
    "encoded_inputs, encoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beca1cdd-a20b-4669-9fd8-d74cbdff77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   616,  1438,   318,  5616,    85,  2047,    13, 39662, 15887,\n",
       "          4538,  1368, 27702]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_targets = generate_tokens_simple(\n",
    "    model = model,\n",
    "    idx = encoded_inputs,\n",
    "    max_num_tokens=5,\n",
    "    context_size = GPT_Config_124M[\"context_length\"]\n",
    ")\n",
    "predicted_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8927ed9-1bce-4418-af90-a26583cf986d",
   "metadata": {},
   "source": [
    "## Training and Validation loss - Real Dataset(the verdict story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68207109-094f-4455-9800-3684ebaba603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\\n\\n\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it\\'s going to send the value of my picture \\'way up; but I don\\'t think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing\\'s lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn\\'s \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\\n\\nWell!--even through th'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data \n",
    "\n",
    "with open(r\"C:\\Users\\nyasa\\Downloads\\BUILDING LLM FROM SCRATCH\\Stage1\\1.Data Preparation and Sampling\\1.Tokenization\\1.Word-Based-Tokenization\\the-verdict.txt\", \"r\") as f:\n",
    "    text_data = f.read()\n",
    "text_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17ecaee1-9620-4ae1-949c-62e3da99dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text has charachters =  20479\n"
     ]
    }
   ],
   "source": [
    "# Checking number of charachters\n",
    "\n",
    "print(\"Sample text has charachters = \", len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f27a8341-ee26-40ec-86d5-68b12785085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text has tokens =  5145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, torch.Size([5145]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "token_ids = encoder(text_data).squeeze(0)\n",
    "print(\"Sample text has tokens = \", len(token_ids)), token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b56dfa29-823c-4ce2-b7bd-727f563fb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the dataloader - input pair targets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class InputPairTargetsV1:\n",
    "    def __init__(self, text, context_length, stride):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        for i in range(0, len(token_ids)-context_length, stride):\n",
    "            input_chunk = token_ids[i: context_length+i]\n",
    "            target_chunk = token_ids[i+1:context_length+i+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def dataloader(text, context_length=256, stride=128, batch_size=4, workers=0, drop=True, shuffle=True):\n",
    "    dataset = InputPairTargetsV1(text, context_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset = dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = workers,\n",
    "        drop_last = drop\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09101833-0030-40cd-8f37-0f386e9c8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "validation_data = text_data[split_idx:]\n",
    "# Sanity check \n",
    "if len(tokenizer.encode(text_data)) * train_ratio < GPT_Config_124M[\"context_length\"]:\n",
    "    print(\"Not  enough tokens for the training dataloader try a smaller context length or increase the training ratio\")\n",
    "if len(tokenizer.encode(text_data)) * (1 - train_ratio) < GPT_Config_124M[\"context_length\"]:\n",
    "    print(\"Not  enough tokens for the validation dataloader try a smaller context length or reduce the training ratio\")    \n",
    "# Creating the train and validation dataloders\n",
    "torch.manual_seed(42)\n",
    "train_dataloader = dataloader(\n",
    "    train_data,\n",
    "    context_length = GPT_Config_124M[\"context_length\"],\n",
    "    stride = GPT_Config_124M[\"context_length\"],\n",
    "    batch_size = 2\n",
    ")\n",
    "validation_dataloader = dataloader(\n",
    "    validation_data,\n",
    "    context_length = GPT_Config_124M[\"context_length\"],\n",
    "    stride = GPT_Config_124M[\"context_length\"],\n",
    "    batch_size = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afa306bb-1a28-473b-abc2-11af66e8794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Number of batches =  9\n",
      "Validation dataloader\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Number of batches =  9\n"
     ]
    }
   ],
   "source": [
    "# Making sure the data is loaded correctly \n",
    "\n",
    "print(\"Train dataloader\")\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"Number of batches = \", len(train_dataloader))    \n",
    "print(\"Validation dataloader\")\n",
    "for x, y in validation_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"Number of batches = \", len(train_dataloader))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd570f3a-ccc0-450b-ad86-4f6e26bc2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5145\n"
     ]
    }
   ],
   "source": [
    "# Optional Second check\n",
    "\n",
    "number_of_elements = 0\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    number_of_elements += input_batch.numel()\n",
    "for input_batch, target_batch in validation_dataloader:\n",
    "    number_of_elements += input_batch.numel()\n",
    "print(number_of_elements, len(tokenizer.encode(text_data)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b71ba7-044a-45cf-8c2a-2b958d7e5e6e",
   "metadata": {},
   "source": [
    "## Calculating the loss of batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "088a727a-2392-4cdd-bb56-866a74f1634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of a single batch\n",
    "def calculateLossBatch(input_batch, target_batch, model, device):\n",
    "    # Setting the device\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    # Getting the logits for the input batch\n",
    "    logits = model(input_batch)\n",
    "    # Getting the loss\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "    \n",
    "# Calculating the cumulative loss of all the batches\n",
    "def calculateTotalLoss(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    # No batches present\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    # If number of batches is not given\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        # If number of batches specified is more than the number of batches in the dataloader reduce\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    # Iterate the dataloader\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calculateLossBatch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    # Calculatin the mean loss            \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "121a0b56-ca78-4b3a-ab37-628c102bbd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the device \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d48fc7f-6719-4fd6-becb-630b1528aa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layerNorm1): LayerNorm()\n",
       "      (layerNorm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting te model to the available device\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc652f30-3df1-41ac-9f4d-9022cc4f82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  11.005153867933485\n",
      "Validation loss =  11.034592628479004\n"
     ]
    }
   ],
   "source": [
    "# Calculating the loss\n",
    "\n",
    "torch.manual_seed(42)\n",
    "with torch.no_grad():\n",
    "    train_loss = calculateTotalLoss(train_dataloader, model, device)\n",
    "    validation_loss = calculateTotalLoss(validation_dataloader, model, device)\n",
    "print(\"Training loss = \", train_loss)    \n",
    "print(\"Validation loss = \", validation_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809d17-7fcf-43a0-a0dc-20876180e7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
