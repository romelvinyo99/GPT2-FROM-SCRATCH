{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd3793c-1717-48e6-99cd-e52572eb31d5",
   "metadata": {},
   "source": [
    "## ---------------------------------------Multi-Head Attention---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda891aa-f7ec-4a12-8ced-b5fa653f0fea",
   "metadata": {},
   "source": [
    "- In theory multi-head attention involves creating mutiple instances of the casual attention attention mechanism and concatenating their outputs\n",
    "- In code I did this by implementing a simple multi-head attention wrapper class that stacks mutiple instances of my previous casual attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da77318a-76cf-4069-bdc3-0b4093f65bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the token embeddings - Randomized\n",
    "import torch\n",
    "output_dim = 3\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your    # X1\n",
    "    [0.55, 0.87, 0.66], # journey # x2\n",
    "    [0.57, 0.85, 0.64], # begins  # X3\n",
    "    [0.22, 0.58, 0.33], # with    # X4\n",
    "    [0.77, 0.25, 0.10], # one     # X5\n",
    "    [0.05, 0.80, 0.55] # step     # X6\n",
    "])  \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388bcdd6-8455-4df7-b75f-bcbf381651e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the input structure\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb57372-46a8-4f60-8297-809cb5908777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casual Attention Class\n",
    "from torch import nn\n",
    "class CasualAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout_rate, context_length, bias_units=False):\n",
    "        super().__init__()\n",
    "        # Defining the key query value weights\n",
    "        self.w_key = torch.nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_query = torch.nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.w_value = torch.nn.Linear(d_in, d_out, bias=bias_units)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) # new\n",
    "        # Creating the masking foundation\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        # Remember we are dealing with batches\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # Getting the key query value matrices\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        # Getting the attention scores - we reshape the inner dimensions in the transpose\n",
    "        attention_scores = queries @ keys.transpose(1, 2)\n",
    "        # Upper triangular infinity mask - modify the tensor in place\n",
    "        attention_scores.masked_fill_(\n",
    "            # Slicing the mask to match the current input  \n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        # Scaling attention scores\n",
    "        scaled_attention_scores = attention_scores / keys.shape[-1]**0.5\n",
    "        # Calculating the attention weight\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, dim=-1)\n",
    "        # Dropout layer\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # Calculating the context vectors\n",
    "        context_vectors = attention_weights @ values\n",
    "        return context_vectors        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46376c51-c8cc-48e5-a53d-333cb5392f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5242,  0.5078],\n",
       "          [-0.6026,  0.5712],\n",
       "          [-0.6281,  0.5896],\n",
       "          [-0.5521,  0.5221],\n",
       "          [-0.4230,  0.4001],\n",
       "          [-0.3174,  0.2891]],\n",
       " \n",
       "         [[-0.5242,  0.5078],\n",
       "          [-0.6026,  0.5712],\n",
       "          [-0.3963,  0.3732],\n",
       "          [-0.5521,  0.5221],\n",
       "          [-0.4848,  0.4248],\n",
       "          [-0.4326,  0.3966]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([2, 6, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instatiation using our batch sample\n",
    "batches, context_length, dimensions = batch.shape\n",
    "ca = CasualAttentionV1(d_in=3, d_out=2, dropout_rate=0.1 ,context_length=context_length)\n",
    "context_vectors = ca(batch)\n",
    "context_vectors, context_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0441c679-10e7-41c4-8ce9-5352bfa4856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from mel|Hello from bob|Hello from chel\n"
     ]
    }
   ],
   "source": [
    "# Example of a wrapper class \n",
    "\n",
    "# Base class\n",
    "class Worker:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def get_result(self):\n",
    "        return f\"Hello from {self.name}\"    \n",
    "# Wrapper class\n",
    "class Manager:\n",
    "    def __init__(self, names):\n",
    "        # This creates a list of instances\n",
    "        self.workers = [Worker(name) for name in names]\n",
    "    def get_combined_result(self):\n",
    "        # For each instance combine the output\n",
    "        return \"|\".join(worker.get_result() for worker in self.workers)\n",
    "\n",
    "# Initialization \n",
    "names = [\"mel\", \"bob\", \"chel\"]\n",
    "manager = Manager(names)\n",
    "print(manager.get_combined_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2acd6732-13c5-495c-b07d-bb9b3e2abe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout_rate, num_heads, bias_units=False):\n",
    "        super().__init__()\n",
    "        # Creating a list of instances\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualAttentionV1(d_in, d_out, dropout_rate, context_length, bias_units) for _ in range(num_heads)]\n",
    "        )\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Concatenating the result of each instance\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0e242-0427-4643-9119-0c5f2156ce5a",
   "metadata": {},
   "source": [
    "## Notes - On the shape of the final output \n",
    "\n",
    "Let the context_vector.shape for each head = (6, 2)\n",
    "\n",
    "The shape of the final context vector = (6, 2 x number of heads)\n",
    "\n",
    "Let number of head  =  2 final context vector = (6, 2x2) = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6f6978-b234-4f79-9529-658a605d6427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6709,  0.4473,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0252,  0.0927],\n",
       "          [-0.7199,  0.2780,  0.1457,  0.1900],\n",
       "          [-0.4445,  0.1822,  0.1457,  0.1629],\n",
       "          [-0.6189,  0.1582,  0.1778,  0.1674],\n",
       "          [-0.4527,  0.1411,  0.1382,  0.1361]],\n",
       " \n",
       "         [[-0.6709,  0.4473, -0.0491,  0.1806],\n",
       "          [-0.7067,  0.3251, -0.0252,  0.0927],\n",
       "          [-0.4755,  0.2188,  0.1457,  0.1900],\n",
       "          [-0.4438,  0.1858,  0.1457,  0.1629],\n",
       "          [-0.4091,  0.0465,  0.1778,  0.1674],\n",
       "          [-0.3942,  0.1231,  0.0991,  0.1052]]], grad_fn=<CatBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the final context vector\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.2, 2)\n",
    "context_vector = mha(batch)\n",
    "context_vector, context_vector.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
