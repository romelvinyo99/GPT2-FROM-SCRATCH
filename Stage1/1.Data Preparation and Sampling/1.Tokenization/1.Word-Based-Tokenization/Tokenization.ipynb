{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c3975e-81a3-4d11-8bb4-22a9ff3ae78f",
   "metadata": {},
   "source": [
    "## -----------------------------WORD-BASED-TOKENIZATION-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec10d80-474c-48e6-9480-634b764ec953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of contents in our text file: \n",
      " I HAD always thought Jack Gisbur\n",
      "\n",
      "Length of charachters in our raw text file = 20479 charachters\n"
     ]
    }
   ],
   "source": [
    "# Reading the text file\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf=8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"Example of contents in our text file: \\n {raw_text[:32]}\\n\")    \n",
    "print(f\"Length of charachters in our raw text file = {len(raw_text)} charachters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8cbd1-7040-4db4-aaa9-8e2b83d7d620",
   "metadata": {},
   "source": [
    "## STEP 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a00e7-ff7f-460b-abc6-ae7a9694959b",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n",
    "- The following regular expression will split on whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b272119-0549-4e4c-a2b1-8f9135c5d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of splitting where white-spaces and punctuations are encountered\n",
    "\n",
    "import re\n",
    "# Creating tokens for words and punctuations\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r\"([,.]|\\s)\", text)\n",
    "# Removing the whitespaces\n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416d7ab3-b0c3-41cd-92e8-7a8867ffa5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inclusion of more punctuation marks using re\n",
    "\n",
    "result = re.split(r\"([,.:;_!]|\\s)\", text)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555c949f-06f1-411a-a0e6-cad7fab4df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nyasa\\Downloads\\ANACONDA\\envs\\nlp\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using spacy - just simplier\n",
    "import spacy\n",
    "def tokens(text):\n",
    "    tokens = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            tokens.append(token.text)\n",
    "    return tokens\n",
    "print(tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10c7033-516f-4c8e-af80-cf3839dda884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4713"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of our full raw text(use render or spacy) - i love spacy \n",
    "\n",
    "preprocessed = tokens(raw_text)\n",
    "# Checking the length of our tokens\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfbc16-3cde-4c91-a119-4e0f6dfc6c50",
   "metadata": {},
   "source": [
    "## STEP 2: Creating Unique Token ID's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8d5f1-b90b-476d-9d7c-1122b195b2e8",
   "metadata": {},
   "source": [
    " - This is just basically creating a vocabulary for our tokens\n",
    " - Sort the tokens in alphabetical order then give them numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5435e23f-2aa1-40dd-b116-d98f4fc31cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting data - unique charachters\n",
    "\n",
    "all_unique_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f932fbb-527e-41bd-9b81-8dd123bb94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary\n",
    "\n",
    "vocab = {char:integer for integer, char in enumerate(all_unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1532144d-4daf-4afc-b022-f21deefbd467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n\\n', 0)\n",
      "('!', 1)\n",
      "('\"', 2)\n",
      "(\"'\", 3)\n",
      "(\"'d\", 4)\n",
      "(\"'re\", 5)\n",
      "(\"'s\", 6)\n",
      "(\"'ve\", 7)\n",
      "('(', 8)\n",
      "(')', 9)\n",
      "(',', 10)\n"
     ]
    }
   ],
   "source": [
    "# Checking some values in our dictionary\n",
    "\n",
    "for i, (word, Id) in enumerate(vocab.items()):\n",
    "    if i>10:\n",
    "        break\n",
    "    print((word, Id))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a7155-288f-4ab0-b92a-108d3ee38c15",
   "metadata": {},
   "source": [
    "## Step 3: Creating a encode and decode method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a081f0-3f36-414c-a80d-f177fcf0809d",
   "metadata": {},
   "source": [
    " - Encode = text >> tokens >> token id\n",
    " - Decode = token id >> tokens >> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3df05d0a-94e6-4ef5-8dfe-ee2550e53522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {ids:word for word, ids in vocab.items()}\n",
    "\n",
    "    def encode(self, raw_text):\n",
    "        preprocessed = tokens(raw_text)\n",
    "        ids = [self.str_to_int[i] for i in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "b = SimpleTokenizerV1(vocab)\n",
    "encoded_text = b.encode(raw_text)\n",
    "decoded_text = b.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48231573-2c73-4b3d-b37b-0c16dcfca0fb",
   "metadata": {},
   "source": [
    "##  Step 4: Adding Special Case Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a468ce7-21c3-4ebb-9a62-b5203deb1959",
   "metadata": {},
   "source": [
    "\n",
    "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
    "- Some tokenizers use special tokens to help the LLM with additional context\n",
    "- Some of these special tokens are\n",
    "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
    "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
    "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "- `[UNK]` to represent words that are not included in the vocabulary\n",
    "\n",
    "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
    "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
    "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12af0edf-faca-4256-878b-bb783cd59f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the unique words\n",
    "unique_words = sorted(set(preprocessed))\n",
    "# Adding the \"endoftext\" and \"unknown\"  tokens\n",
    "unique_words.extend([\"EOT\", \"UNK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4968b62-4b48-4a57-8243-a53ce8060692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(preprocessed)) == sorted(list(set(preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "172f506e-2756-487b-8001-e2e3285eff3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New vocabulary\n",
    "\n",
    "vocab = {words:ids for ids, words in enumerate(unique_words)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5c2d405-6830-4bce-bebd-8e48780fbdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger 1140\n",
      "your 1141\n",
      "yourself 1142\n",
      "EOT 1143\n",
      "UNK 1144\n"
     ]
    }
   ],
   "source": [
    "# Testing the new vocabulary extension\n",
    "\n",
    "for key, value in list(vocab.items())[-5:]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec725864-fc69-43ec-bf10-c62e1d9af693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the unknown token\n",
    "\n",
    "class SimpleTokenizerVersion2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {ids:words for words, ids in vocab.items()}\n",
    "    def encode(self, Text):\n",
    "        preprocessed_1 = tokens(Text)\n",
    "        preprocessed_2 = []\n",
    "        for i in preprocessed_1:\n",
    "            if i in self.str_to_int:\n",
    "                preprocessed_2.append(i)\n",
    "            else:\n",
    "                preprocessed_2.append(\"UNK\")\n",
    "        \n",
    "        ids = [self.str_to_int[i] for i in preprocessed_2]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4218738-2d1a-43e1-a33c-c56839e27a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? EOT In the sunlit terraces of the palace.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of implementation of the end of text token\n",
    "\n",
    "tokenizer = SimpleTokenizerVersion2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text12 = \" EOT \".join((text1, text2))\n",
    "text12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "745089fe-b994-4a67-8080-54bdb9bf323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'tea',\n",
       " '?',\n",
       " 'EOT',\n",
       " 'In',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'terraces',\n",
       " 'of',\n",
       " 'the',\n",
       " 'palace',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the tokens after tokenization\n",
    "\n",
    "tokens(text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b14bb65-ca8c-48a5-92a4-25eeea0b7323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the postion of the end of text\n",
    "\n",
    "f = [i for i, v in enumerate(tokens(text12)) if v == \"EOT\"]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c95c0e4f-29d2-483b-b88e-3cccf9b10ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1144,\n",
       " 10,\n",
       " 367,\n",
       " 1139,\n",
       " 636,\n",
       " 989,\n",
       " 16,\n",
       " 1143,\n",
       " 61,\n",
       " 1003,\n",
       " 971,\n",
       " 998,\n",
       " 733,\n",
       " 1003,\n",
       " 1144,\n",
       " 13]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there is a token id for end of text and unkowns --> They are both present \n",
    "\n",
    "tokenizer.encode(text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b14588b7-eb97-454a-ad7e-cd0e37b0b3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[1144][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db122703-93f5-4b04-8bbe-44c1e29c13be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK, do you like tea? EOT In the sunlit terraces of the UNK.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(text12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
