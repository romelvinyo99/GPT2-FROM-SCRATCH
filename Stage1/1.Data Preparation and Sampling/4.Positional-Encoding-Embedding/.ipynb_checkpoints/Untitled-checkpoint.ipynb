{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30be606-043e-4faa-a7b5-a4edbefacba3",
   "metadata": {},
   "source": [
    "## --------------Position-Embedding(Word Positions)-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b956ef6-8781-4618-aaf5-c739c3624893",
   "metadata": {},
   "source": [
    "## 1. Creating input-pair targets dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba3c19f6-dc33-463b-8ece-9aa7f9f332bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisb'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading raw text file\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "raw_text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2b447a67-16b0-433d-ac31-1496ef539b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "becafd31-a025-4a14-9216-fafdceba9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Input-Pair targets class\n",
    "import tiktoken\n",
    "\n",
    "class InputTargetV1:\n",
    "    def __init__(self, text, context_size, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids)-context_size, stride):\n",
    "            input_chunk = token_ids[i: i+context_size]\n",
    "            target_chunk = token_ids[i+1: i+context_size+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]   \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Creating the dataloader\n",
    "def createDataloader(text, context_size=4, stride=1, shuffle=True, drop_last=True, batch_size=4, workers=0):\n",
    "    dataset = InputTargetV1(text, context_size, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset = dataset, \n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = workers,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b4a1d450-3163-4a26-b26b-60f159dad4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting input-pair targets dataloader\n",
    "\n",
    "dataloader = createDataloader(text=raw_text, context_size=4, stride=4, batch_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "12f105c2-b30e-4f76-8c23-e24de5e0de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "[tensor([[41186, 39614,  1386,    11],\n",
      "        [14005,  1801,  2093, 41160],\n",
      "        [  925,   257,  1207,  8344],\n",
      "        [24357,  1871, 12734,   379],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  470,  6842,   407,   284],\n",
      "        [  355,  1752,   530,   550],\n",
      "        [  306,    11,   475,   465]]), tensor([[39614,  1386,    11,   287],\n",
      "        [ 1801,  2093, 41160,    11],\n",
      "        [  257,  1207,  8344,   803],\n",
      "        [ 1871, 12734,   379,  1123],\n",
      "        [13476,    11,   339,   550],\n",
      "        [ 6842,   407,   284,   423],\n",
      "        [ 1752,   530,   550,   890],\n",
      "        [   11,   475,   465,  2951]])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch_data in enumerate(dataloader):\n",
    "    # Each batch has 8 input samples and 8 output samples\n",
    "    print(len(batch_data[0])), print(len(batch_data[1]))\n",
    "    print(batch_data)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c5865-b52a-4df5-8827-690b007ef60c",
   "metadata": {},
   "source": [
    "## 4. Embedding-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b113e904-9039-42da-b5d8-89c9a655f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eaf939e3-6305-4615-8d5f-26387878b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5178, -0.1704, -0.9528,  ..., -2.5087, -0.0688,  0.5584],\n",
      "        [ 0.8835,  1.5265, -0.0493,  ...,  0.8239,  0.4073,  1.9341],\n",
      "        [-1.0035,  0.2318,  0.0290,  ..., -2.0758, -0.8288, -0.6821],\n",
      "        ...,\n",
      "        [-1.2668, -0.7697,  0.7771,  ..., -1.3613,  1.6117,  0.7102],\n",
      "        [ 1.0284, -1.3671, -0.4611,  ..., -1.7949,  0.1261,  0.2523],\n",
      "        [-0.2564,  0.9530,  0.6455,  ..., -1.4528, -1.1175, -0.3538]],\n",
      "       requires_grad=True) torch.Size([50257, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initializing the embedding layer\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# Initializing the embedding layer\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight, embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dff04-3c36-4694-abe7-3a61cb2f1eea",
   "metadata": {},
   "source": [
    "## Generating the Embedding vector for each input\n",
    "\n",
    "This will generate the 256 dimension vector for each token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef6877bd-76a9-4d4e-becb-8b5ae04492c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6690,  1.1381,  0.9624,  ...,  0.7295, -0.3945,  1.3765],\n",
      "         [ 0.1877,  1.0852, -1.3978,  ..., -0.9610, -0.3061, -0.6785],\n",
      "         [ 1.1640, -0.6520,  0.5414,  ..., -0.7368, -0.1855, -0.4861],\n",
      "         [ 0.7021,  1.7719, -0.0818,  ..., -0.2265,  1.2508,  0.3222]],\n",
      "\n",
      "        [[-1.1051, -0.4490,  0.4626,  ..., -0.2854,  0.6049,  0.4611],\n",
      "         [ 0.0213, -0.7798,  0.8014,  ..., -0.3454,  0.1818, -1.9946],\n",
      "         [ 0.1106, -0.5914,  0.5721,  ...,  1.3130,  2.7730,  0.8521],\n",
      "         [-0.4169,  0.2237, -0.2018,  ..., -0.5885, -0.1785,  0.4377]],\n",
      "\n",
      "        [[ 0.6272, -1.0081,  1.5113,  ...,  2.1918, -0.2853, -0.7202],\n",
      "         [ 0.5513, -1.1779,  1.2864,  ..., -0.6121, -0.9824,  1.7844],\n",
      "         [ 1.5172, -0.4209,  0.4708,  ...,  0.2462,  0.5986,  1.7397],\n",
      "         [-1.1983,  2.0755,  1.4999,  ...,  1.6147,  0.0358, -0.4715]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1334,  0.3464, -0.1582,  ..., -1.4435, -0.8234,  0.5223],\n",
      "         [-0.5353,  0.1816,  1.3806,  ..., -0.7710,  0.6232,  0.3359],\n",
      "         [-1.3859, -0.4086,  0.8404,  ...,  0.3230,  0.8818, -2.1244],\n",
      "         [-1.3816, -1.4074,  0.3721,  ...,  0.1325, -0.2960, -0.4910]],\n",
      "\n",
      "        [[ 0.9045, -0.6411,  0.8065,  ..., -1.3673, -0.8887,  0.7073],\n",
      "         [ 1.3547,  0.3526, -1.1725,  ...,  0.5185,  0.2708,  0.2724],\n",
      "         [ 1.1947, -2.3067,  0.4627,  ..., -1.9447,  1.1668, -0.2238],\n",
      "         [-1.1614,  0.5252, -0.3794,  ..., -0.4206,  0.0480,  0.6994]],\n",
      "\n",
      "        [[ 0.5282,  0.7171,  0.6434,  ...,  0.4125,  1.6813, -1.6492],\n",
      "         [ 0.8327, -1.3566,  1.6745,  ..., -0.1944,  0.7867, -2.3171],\n",
      "         [ 1.0693,  0.1786,  0.8991,  ...,  0.0987, -1.0712,  0.9626],\n",
      "         [-0.2707,  0.5075,  0.3857,  ..., -1.2161, -0.2123, -1.6061]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 6487,   326,  3088,   284],\n",
      "        [17047,  8167, 32545,    13],\n",
      "        [ 2084,   553,   339,   531],\n",
      "        [  319, 12036,    26,   475],\n",
      "        [  314,  1043,   607,   523],\n",
      "        [  373,   866,   287,  6245],\n",
      "        [  351,   884,  2784,  9830],\n",
      "        [ 2073,    11,   290,   523]])\n",
      "New batch shape =  torch.Size([8, 4, 256]) Old batch shape =  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example using the first batch of inputs\n",
    "\n",
    "# Getting the first batch\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# Passing the first batch inputs into the embedding layer\n",
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings), print(inputs)\n",
    "print(\"New batch shape = \", token_embeddings.shape, \"Old batch shape = \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15d1df3f-a5e3-4710-bee8-4c94b7fa7ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5178, -0.1704, -0.9528,  ..., -2.5087, -0.0688,  0.5584],\n",
       "        [ 0.8835,  1.5265, -0.0493,  ...,  0.8239,  0.4073,  1.9341],\n",
       "        [-1.0035,  0.2318,  0.0290,  ..., -2.0758, -0.8288, -0.6821],\n",
       "        ...,\n",
       "        [-1.2668, -0.7697,  0.7771,  ..., -1.3613,  1.6117,  0.7102],\n",
       "        [ 1.0284, -1.3671, -0.4611,  ..., -1.7949,  0.1261,  0.2523],\n",
       "        [-0.2564,  0.9530,  0.6455,  ..., -1.4528, -1.1175, -0.3538]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532d8f1-24ef-452c-a1aa-38ce4d4329ec",
   "metadata": {},
   "source": [
    "## Generating the postion embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f120c-2383-4f68-9f8c-a9999ae23de5",
   "metadata": {},
   "source": [
    "- We create an embedding vector for each position\n",
    "\n",
    "\n",
    "- shape = [context_length, output_dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2fb6d18a-ce8b-4ef8-83ea-d67749cda024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the context length from the first batch\n",
    "\n",
    "for i in inputs:\n",
    "       context_size = len(i)\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8e162029-163a-4bf5-a13b-bf7492849a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0901, -1.6436, -0.5845,  ..., -0.8266,  0.4108, -0.3341],\n",
       "         [-0.2589, -1.6159, -0.1921,  ..., -0.9727, -0.7486,  0.7387],\n",
       "         [-0.4493,  0.0408, -2.4012,  ...,  0.4037, -0.3420,  0.0703],\n",
       "         [ 1.1106,  0.5037,  0.5805,  ...,  0.7508, -1.5422,  2.1911]],\n",
       "        requires_grad=True),\n",
       " torch.Size([4, 256]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the postional embedding vectors for each position\n",
    "\n",
    "positional_embedding_layer = torch.nn.Embedding(context_size, output_dim)\n",
    "positional_embedding_layer.weight, positional_embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3c01ece5-1387-4bb4-850c-006aa2373e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1b139c37-2cfe-4900-bfb9-7d4d2a96c722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2589, -1.6159, -0.1921,  0.6388,  0.7441, -1.3314, -1.8576,  0.9419,\n",
       "        -0.8074, -0.9684, -1.7818, -0.7659, -0.2781, -1.2846,  0.5704, -0.5450,\n",
       "        -0.4168, -0.0484,  0.2756,  1.9855, -0.1068, -0.9800, -0.3448,  0.4535,\n",
       "         1.1460, -0.2845,  1.9568, -0.6479,  0.3111,  1.8521, -0.2772, -0.7320,\n",
       "        -0.8943, -1.8134,  0.1192, -0.7232, -1.0843,  0.2610,  0.8612, -0.4366,\n",
       "         1.1077,  0.3016, -0.0544, -1.2166, -0.3323,  1.3721,  0.0737, -1.9004,\n",
       "        -0.7153,  1.6733, -1.2304, -0.7418,  1.3883,  0.5080,  2.0998,  0.0786,\n",
       "         0.2045,  1.0595, -0.6164, -1.7452, -1.4288, -0.0795, -1.3050,  0.5518,\n",
       "         1.6310, -0.0471, -0.2818,  0.4896,  0.6838,  0.8488,  0.5804, -1.5464,\n",
       "        -1.0935, -0.5829,  0.3518,  0.7696, -0.1511, -1.0045, -0.3071,  0.1626,\n",
       "         0.4616,  0.5121, -2.1564, -1.8176,  0.1345,  0.4961, -1.2961,  0.3802,\n",
       "        -1.0648,  0.9077,  0.4614,  0.5925, -1.1326,  0.1554, -0.1843,  0.0914,\n",
       "        -0.3598, -1.0147,  0.3275,  0.0412,  0.7115, -0.6048, -0.9270, -0.6583,\n",
       "        -0.9526, -0.3690, -0.4791,  1.2586, -0.0823, -2.1420, -0.8667, -1.1849,\n",
       "         1.0183, -1.2485,  0.6085, -0.7622,  0.5294,  1.0758,  0.4243, -0.3272,\n",
       "         1.2111, -0.5297, -1.3193,  0.9916, -0.0146,  2.6274,  1.8298,  1.5841,\n",
       "         1.9337,  1.4545,  0.0602,  1.7985,  1.5023,  0.2793, -0.7088,  0.5030,\n",
       "         1.2312,  0.3389,  1.3495, -2.2420,  0.8069, -0.0967,  0.5593,  1.2270,\n",
       "         0.5788, -0.7992,  0.6796,  0.9468, -0.7387,  3.0378,  2.9489, -0.5564,\n",
       "         0.7109,  0.1711, -0.3634,  0.9099,  0.7718, -1.0033, -0.1957,  0.2677,\n",
       "         0.0143, -0.6195,  0.4974,  1.2317, -1.5206,  1.2025, -1.9075,  0.9784,\n",
       "         0.5177, -1.6555,  0.1974,  1.0756,  2.5448,  0.4434, -1.2168, -0.2197,\n",
       "        -1.0988,  0.0634, -0.2368, -0.1839, -1.1284,  0.2903, -1.1904, -1.0995,\n",
       "        -0.1286, -0.9980,  0.8020,  0.6066, -0.7729,  0.0445, -0.0789, -0.4923,\n",
       "         0.5729,  1.5274,  0.8367, -0.2767, -1.7261, -2.4071, -1.1361,  0.9442,\n",
       "        -1.0361,  1.5483,  1.0635, -0.3616,  2.5961,  0.7146,  0.9093,  0.5009,\n",
       "         0.0065, -0.1583,  0.5117,  1.4659,  1.7947,  0.7514,  0.6591, -0.6833,\n",
       "         0.1298, -2.0597, -0.2245,  0.0409, -1.5756,  0.1869, -0.7930, -0.4648,\n",
       "        -0.6715, -1.4134, -0.5650, -1.0600,  2.1652,  0.1223,  0.7354,  0.3540,\n",
       "         0.2906,  0.6010,  0.2280, -0.4466, -1.6965, -0.9529, -0.1186,  0.6839,\n",
       "         1.4516, -0.4141,  1.3051, -0.5965, -0.0568, -1.6444, -0.6475, -0.2719,\n",
       "         0.0521, -0.0665,  2.5526,  0.2047,  0.5061, -0.9727, -0.7486,  0.7387],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_layer(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "99b633a1-8470-4007-a228-77b5e5422fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0901, -1.6436, -0.5845,  ..., -0.8266,  0.4108, -0.3341],\n",
       "        [-0.2589, -1.6159, -0.1921,  ..., -0.9727, -0.7486,  0.7387],\n",
       "        [-0.4493,  0.0408, -2.4012,  ...,  0.4037, -0.3420,  0.0703],\n",
       "        [ 1.1106,  0.5037,  0.5805,  ...,  0.7508, -1.5422,  2.1911]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the positional embeddings for the positions\n",
    "# We will pass the ids which are four thats why we have the torch.arange() method\n",
    "\n",
    "positional_embeddings = positional_embedding_layer(torch.arange(context_size))\n",
    "positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1b8c6362-7e35-4f79-9ea3-f17e0f9460f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape, positional_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c7be9-204b-43f4-8e07-780d99bcd42d",
   "metadata": {},
   "source": [
    "## Creating input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2141868-cfb2-4c59-a35c-961c7e3489e2",
   "metadata": {},
   "source": [
    "Input embeddings = positional embeddings + token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2870c493-3f1f-42f5-b72f-497b16da8178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember this token embeddings are for the first batch - we used an iterator on the dataloader\n",
    "\n",
    "input_embeddings = token_embeddings + positional_embeddings\n",
    "input_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
